{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import kaggle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "root_dir = \"../..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir_path = pathlib.Path(root_dir)\n",
    "data_dir_path = root_dir_path / \"data\"\n",
    "raw_dir_path = data_dir_path / \"raw\"\n",
    "sample_submission_path = raw_dir_path / \"sample_submission.csv\"\n",
    "models_dir_path = root_dir_path / \"models\"\n",
    "model_path = models_dir_path / \"lgbm_reg.joblib\"\n",
    "prediction_path = models_dir_path / \"prediction.parquet\"\n",
    "submission_path = models_dir_path / \"submission_uncertainty.csv.gz\"\n",
    "src_dir_path = root_dir_path / \"src\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(str(src_dir_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(sample_submission_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pd.read_parquet(prediction_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = prediction[\"d\"].str[2:]\n",
    "d = d.astype(\"int\")\n",
    "d -= train_days + 1\n",
    "\n",
    "is_evaluation = d >= test_days\n",
    "d %= test_days\n",
    "\n",
    "prediction[\"d\"] = d\n",
    "prediction.loc[is_evaluation, \"id\"] = prediction.loc[is_evaluation, \"id\"].str.replace(\n",
    "    \"_validation\", \"_evaluation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.pivot(prediction, index=\"id\", columns=\"d\", values=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.columns = [\"id\"] + [f\"F{i + 1}\" for i in range(test_days)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.merge(sample_submission[\"id\"], submission, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This process will need to be fixed later\n",
    "is_validation = submission[\"id\"].str.endswith(\"_validation\")\n",
    "submission = submission[is_validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[[\"item_id\", \"store_id\"]] = submission[\"id\"].str.extract(\n",
    "    r\"(\\w+_\\d+_\\d+)_(\\w+_\\d+)_\\w+\"\n",
    ")\n",
    "submission[\"dept_id\"] = submission[\"item_id\"].str.extract(r\"(\\w+_\\d+)_\\d+\")\n",
    "submission[\"cat_id\"] = submission[\"dept_id\"].str.extract(r\"(\\w+)_\\d+\")\n",
    "submission[\"state_id\"] = submission[\"store_id\"].str.extract(r\"(\\w+)_\\d+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratios(coef=0.15):\n",
    "    qs2 = np.log(qs / (1 - qs)) * coef\n",
    "    ratios = stats.norm.cdf(qs2)\n",
    "    ratios /= ratios[4]\n",
    "    ratios = pd.Series(ratios, index=qs)\n",
    "\n",
    "    return ratios.round(3)\n",
    "\n",
    "\n",
    "def quantile_coefs(q, level):\n",
    "    ratios = level_coef_dict[level]\n",
    "\n",
    "    return ratios.loc[q].values\n",
    "\n",
    "\n",
    "def get_group_preds(pred, level):\n",
    "    df = pred.groupby(level)[cols].sum()\n",
    "    q = np.repeat(qs, len(df))\n",
    "    df = pd.concat([df] * 9, axis=0, sort=False)\n",
    "\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    df[cols] *= quantile_coefs(q, level)[:, None]\n",
    "\n",
    "    if level != \"id\":\n",
    "        df[\"id\"] = [\n",
    "            f\"{lev}_X_{q:.3f}_validation\" for lev, q in zip(df[level].values, q)\n",
    "        ]\n",
    "    else:\n",
    "        df[\"id\"] = [\n",
    "            f\"{lev.replace('_validation', '')}_{q:.3f}_validation\"\n",
    "            for lev, q in zip(df[level].values, q)\n",
    "        ]\n",
    "\n",
    "    df = df[[\"id\"] + list(cols)]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_couple_group_preds(pred, level1, level2):\n",
    "    df = pred.groupby([level1, level2])[cols].sum()\n",
    "    q = np.repeat(qs, len(df))\n",
    "    df = pd.concat([df] * 9, axis=0, sort=False)\n",
    "\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    df[cols] *= quantile_coefs(q, (level1, level2))[:, None]\n",
    "    df[\"id\"] = [\n",
    "        f\"{lev1}_{lev2}_{q:.3f}_validation\"\n",
    "        for lev1, lev2, q in zip(df[level1].values, df[level2].values, q)\n",
    "    ]\n",
    "    df = df[[\"id\"] + list(cols)]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "submission[\"_all_\"] = \"Total\"\n",
    "\n",
    "qs = np.array([0.005, 0.025, 0.165, 0.25, 0.5, 0.75, 0.835, 0.975, 0.995])\n",
    "\n",
    "# coef between 0.05 and 0.24 is used, probably suboptimal values for now\n",
    "level_coef_dict = {\n",
    "    \"id\": get_ratios(coef=0.3),\n",
    "    \"item_id\": get_ratios(coef=0.15),\n",
    "    \"dept_id\": get_ratios(coef=0.08),\n",
    "    \"cat_id\": get_ratios(coef=0.07),\n",
    "    \"store_id\": get_ratios(coef=0.08),\n",
    "    \"state_id\": get_ratios(coef=0.07),\n",
    "    \"_all_\": get_ratios(coef=0.05),\n",
    "    (\"state_id\", \"item_id\"): get_ratios(coef=0.19),\n",
    "    (\"state_id\", \"dept_id\"): get_ratios(coef=0.1),\n",
    "    (\"store_id\", \"dept_id\"): get_ratios(coef=0.11),\n",
    "    (\"state_id\", \"cat_id\"): get_ratios(coef=0.08),\n",
    "    (\"store_id\", \"cat_id\"): get_ratios(coef=0.1),\n",
    "}\n",
    "\n",
    "levels = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\", \"_all_\"]\n",
    "couples = [\n",
    "    (\"state_id\", \"item_id\"),\n",
    "    (\"state_id\", \"dept_id\"),\n",
    "    (\"store_id\", \"dept_id\"),\n",
    "    (\"state_id\", \"cat_id\"),\n",
    "    (\"store_id\", \"cat_id\"),\n",
    "]\n",
    "cols = [f\"F{i}\" for i in range(1, 29)]\n",
    "\n",
    "df = []\n",
    "\n",
    "for level in levels:\n",
    "    df.append(get_group_preds(submission, level))\n",
    "\n",
    "for level1, level2 in couples:\n",
    "    df.append(get_couple_group_preds(submission, level1, level2))\n",
    "\n",
    "df = pd.concat(df, axis=0, sort=False)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df = pd.concat([df, df], axis=0, sort=False)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.loc[df.index >= len(df.index) // 2, \"id\"] = df.loc[\n",
    "    df.index >= len(df.index) // 2, \"id\"\n",
    "].str.replace(\"_validation$\", \"_evaluation\")\n",
    "\n",
    "submission = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "submission.to_csv(submission_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle.api.competition_submit(\n",
    "    submission_path,\n",
    "    message=str(model.best_score),\n",
    "    competition=\"m5-forecasting-uncertainty\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
